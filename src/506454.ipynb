{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import permutations, combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **1)** Five layer perceptron regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the data into a pandas DataFrame, and get a scikit-learn compatible dataset. Use\n",
    "the “target” column as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "full_data_part1 = pd.read_csv(\"Part 1.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Using only half of the data\n",
    "partial_data = full_data_part1.head(int(full_data_part1.shape[0]/2))\n",
    "\n",
    "features = [x for x in partial_data.columns.drop(\"target\")]\n",
    "\n",
    "X = partial_data[features]\n",
    "y = partial_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314826</td>\n",
       "      <td>0.876050</td>\n",
       "      <td>-0.685288</td>\n",
       "      <td>0.604524</td>\n",
       "      <td>-0.868619</td>\n",
       "      <td>-0.671283</td>\n",
       "      <td>-59.177636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.662332</td>\n",
       "      <td>0.745420</td>\n",
       "      <td>-0.423713</td>\n",
       "      <td>0.179228</td>\n",
       "      <td>-1.349615</td>\n",
       "      <td>0.355827</td>\n",
       "      <td>-11.643998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.448170</td>\n",
       "      <td>0.065115</td>\n",
       "      <td>0.107046</td>\n",
       "      <td>0.459968</td>\n",
       "      <td>1.302039</td>\n",
       "      <td>-0.758131</td>\n",
       "      <td>35.562439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.199397</td>\n",
       "      <td>-1.109276</td>\n",
       "      <td>-0.003139</td>\n",
       "      <td>-0.916933</td>\n",
       "      <td>-0.325098</td>\n",
       "      <td>-0.655460</td>\n",
       "      <td>-18.147836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.066389</td>\n",
       "      <td>1.762515</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>-1.308726</td>\n",
       "      <td>-0.631064</td>\n",
       "      <td>-0.038181</td>\n",
       "      <td>-42.328718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_0    feat_1    feat_2    feat_3    feat_4    feat_5     target\n",
       "0  0.314826  0.876050 -0.685288  0.604524 -0.868619 -0.671283 -59.177636\n",
       "1  0.662332  0.745420 -0.423713  0.179228 -1.349615  0.355827 -11.643998\n",
       "2 -0.448170  0.065115  0.107046  0.459968  1.302039 -0.758131  35.562439\n",
       "3  1.199397 -1.109276 -0.003139 -0.916933 -0.325098 -0.655460 -18.147836\n",
       "4 -0.066389  1.762515  0.560241 -1.308726 -0.631064 -0.038181 -42.328718"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Make a 70%/30% split of the dataset for training and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho de treino: (3500, 6)\n",
      "Tamanho de teste: (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "print(f\"Tamanho de treino: {X_train.shape}\")\n",
    "print(f\"Tamanho de teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Using numpy, create a scikit-learn regressor that implements a multilayer perceptron\n",
    "architecture with 5 hidden layers.\n",
    "- The dimensionality of each layer is your decision.\n",
    "- Each hidden layer must have a bias unit.\n",
    "- All activations should be the sigmoid function.\n",
    "- You must use the backpropagation algorithm to calculate the derivatives.\n",
    "- Use mini-batch gradient descent to update the weights.\n",
    "- The parameters of the estimator are the following:\n",
    "    - **learning_rate:** A float number that determines the learning rate used for\n",
    "updating the weights on the update step of the gradient descent.\n",
    "    - **batch_size**: An integer that determines the number of datapoints that\n",
    "are included in each mini-batch.\n",
    "    - **epochs**: An integer that determines the number of times the training\n",
    "goes through all the datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class FiveLayerPerceptronRegressor(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.01, batch_size=100, epochs=100, weight_decay=0.1, size_hidden=100):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.size_hidden = size_hidden\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.rng = np.random.default_rng(0)\n",
    "\n",
    "        self.training_losses = np.zeros(self.epochs)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "    def _sigmoid_derivative(self, Z):\n",
    "        x = self._sigmoid(Z)\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def cost_function(self, y, y_hat):\n",
    "        return np.sum(np.square(y_hat - y)) * 1/2 \n",
    "\n",
    "    def cost_gradient(self, y, y_hat):\n",
    "        return (y_hat - y)\n",
    "\n",
    "    def _forward_propagation(self, X):\n",
    "        self.Z = []\n",
    "        self.A = []\n",
    "\n",
    "        for i in range(5):\n",
    "            self.Z.append((X @ self._W[i]))\n",
    "            self.A.append(self._sigmoid(self.Z[i]))\n",
    "            self.A[i] = np.c_[np.ones((self.A[i].shape[0],1)), self.A[i]]\n",
    "            X = self.A[i]\n",
    "            \n",
    "        self.Z.append((self.A[-1] @ self._W[-1]))\n",
    "        self.A.append(self.Z[-1])\n",
    "\n",
    "        return self.A[-1]\n",
    "\n",
    "    # function of backward propagation for regression\n",
    "    def _backward_propagation(self, X, y):\n",
    "        dA = []\n",
    "        dW = []\n",
    "        delta = []\n",
    "        \n",
    "        dA.append(self.cost_gradient(y, self.A[-1]))\n",
    "        delta.append(dA[0])\n",
    "        dW.append((self.A[-2].T @ delta[0]))\n",
    "        \n",
    "        for i in range(5, 1, -1):   \n",
    "            dA.append((delta[-1] @ self._W[i][1:,:].T))\n",
    "            delta.append(dA[-1] * self._sigmoid_derivative(self.Z[i-1]))\n",
    "            dW.append((self.A[i-2].T @ delta[-1]))\n",
    "\n",
    "        dA.append((delta[-1] @ self._W[1][1:,:].T))\n",
    "        delta.append(dA[-1] * self._sigmoid_derivative(self.Z[0]))\n",
    "        dW.append((X.T @ delta[-1]))\n",
    "\n",
    "\n",
    "        dW.reverse()\n",
    "\n",
    "        return dW\n",
    "        \n",
    "    def _weight_update(self, dW, curr):\n",
    "        for i in range(len(dW)):\n",
    "            self._W[i] -= (self.learning_rate * dW[i]) / curr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, m = X.shape\n",
    "\n",
    "        _X = np.c_[np.ones((n,1)), X]\n",
    "        y = y.to_numpy()\n",
    "        _y = y[:,np.newaxis]\n",
    "\n",
    "        self._W = []\n",
    "\n",
    "        self._W.append(np.random.randn(m+1, self.size_hidden))\n",
    "        for i in range(4):\n",
    "            self._W.append(np.random.randn(self.size_hidden+1, self.size_hidden))\n",
    "        self._W.append(np.random.randn(self.size_hidden+1, 1))\n",
    "\n",
    "        n_batches = (n + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for b in range(n_batches):\n",
    "                _X_batch = _X[b * self.batch_size : (b + 1) * self.batch_size]\n",
    "                _y_batch = _y[b * self.batch_size : (b + 1) * self.batch_size]\n",
    "                y_pred = self._forward_propagation(_X_batch)\n",
    "                curr_batch_size = _y_batch.shape[0]\n",
    "                dW = self._backward_propagation(_X_batch, _y_batch)\n",
    "                self._weight_update(dW, curr_batch_size)\n",
    "            y_pred = self._forward_propagation(_X)\n",
    "            self.training_losses[epoch] = self.cost_function(_y, y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n, m = X.shape\n",
    "        _X = np.c_[np.ones((n, 1)), X]\n",
    "        \n",
    "        return self._forward_propagation(_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Train the estimator you implemented using the training set. Use the trained estimator\n",
    "to predict values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-282.32584253]\n",
      " [ -26.72204834]\n",
      " [ -77.44225457]\n",
      " ...\n",
      " [  22.58834868]\n",
      " [ 332.3378897 ]\n",
      " [ 193.07380046]]\n"
     ]
    }
   ],
   "source": [
    "model = FiveLayerPerceptronRegressor(epochs=100, learning_rate=0.01, batch_size=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Use the scikit-learn MLPRegressor estimator. Train it on the training test and generate\n",
    "predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-280.70865491,  -23.87191804,  -76.66268059, ...,   20.62137285,\n",
       "        327.22577662,  192.86382334])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,), \\\n",
    "    learning_rate_init=0.1, activation=\"logistic\",\\\n",
    "    batch_size=100)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_mlp_pred = mlp.predict(X_test)\n",
    "y_mlp_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Compare the performance of both models using the mean squared error metric from\n",
    "scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.952456283813922 9.717988909765909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_pred), mean_squared_error(y_test, y_mlp_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### **2)** multi layers perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "full_data_part2 = pd.read_csv(\"Part 2.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Using only half of the data\n",
    "partial_data_2 = full_data_part2.head(int(full_data_part2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.772441</td>\n",
       "      <td>0.360758</td>\n",
       "      <td>-2.381101</td>\n",
       "      <td>0.087570</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.149460</td>\n",
       "      <td>0.622546</td>\n",
       "      <td>0.373029</td>\n",
       "      <td>0.459658</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.908792</td>\n",
       "      <td>-1.160263</td>\n",
       "      <td>-0.273645</td>\n",
       "      <td>-0.827660</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.776947</td>\n",
       "      <td>0.314343</td>\n",
       "      <td>-2.262319</td>\n",
       "      <td>0.063391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.170471</td>\n",
       "      <td>0.022124</td>\n",
       "      <td>-2.173768</td>\n",
       "      <td>-0.134220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_0    feat_1    feat_2    feat_3  target\n",
       "0 -0.772441  0.360758 -2.381101  0.087570     0.0\n",
       "1  1.149460  0.622546  0.373029  0.459658     0.0\n",
       "2 -1.908792 -1.160263 -0.273645 -0.827660     1.0\n",
       "3 -0.776947  0.314343 -2.262319  0.063391     0.0\n",
       "4 -1.170471  0.022124 -2.173768 -0.134220     0.0"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho de treino: (7000, 4)\n",
      "Tamanho de teste: (3000, 4)\n"
     ]
    }
   ],
   "source": [
    "features_2 = [x for x in partial_data_2.columns.drop(\"target\")]\n",
    "\n",
    "X = partial_data_2[features_2]\n",
    "y = partial_data_2[\"target\"]\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "print(f\"Tamanho de treino: {X_train_2.shape}\")\n",
    "print(f\"Tamanho de teste: {X_test_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "# Class of a neural network for classification\n",
    "class NeuralNetClass(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_dimensions, learning_rate, batch_size\n",
    "                    , epochs):\n",
    "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
    "        self.size_of_hidden = len(hidden_layer_dimensions)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.training_losses = np.zeros(self.epochs)\n",
    "        self.rng = np.random.default_rng(0)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "    def _sigmoid_derivative(self, Z):\n",
    "        return self._sigmoid(Z) * (1.0 - self._sigmoid(Z))\n",
    "\n",
    "    def cost_function(self, y, y_hat):\n",
    "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "    def cost_gradient(self, y, y_hat):\n",
    "        return -np.divide(y, y_hat) + np.divide(1.0 - y, 1.0 - y_hat)\n",
    "\n",
    "    def _forward_propagation(self, X):\n",
    "        self.Z = []\n",
    "        self.A = []\n",
    "\n",
    "        for i in range(self.size_of_hidden):\n",
    "            self.Z.append(np.dot(X, self._W[i]))\n",
    "            self.A.append(self._sigmoid(self.Z[i]))\n",
    "            self.A[i] = np.c_[np.ones((self.A[i].shape[0],1)), self.A[i]]\n",
    "            X = self.A[i]\n",
    "\n",
    "        self.Z.append(np.dot(self.A[-1], self._W[-1]))\n",
    "        self.A.append(self._sigmoid(self.Z[-1]))\n",
    "\n",
    "        return self.A[-1]\n",
    "        \n",
    "\n",
    "    def _backward_propagation(self, X, y):\n",
    "        dA = []\n",
    "        dW = []\n",
    "        dZ = []\n",
    "\n",
    "        dA.append(self.cost_gradient(y, self.A[-1]))\n",
    "        dZ.append(dA[0] * self._sigmoid_derivative(self.Z[-1]))\n",
    "        dW.append(self.A[-2].T @ dZ[0])\n",
    "\n",
    "        for i in range(self.size_of_hidden, 1, -1):\n",
    "            dA.append(dZ[-1] @ self._W[i][1:,:].T)\n",
    "            dZ.append(dA[-1] * self._sigmoid_derivative(self.Z[i-1]))\n",
    "            dW.append(self.A[i-2].T @ dZ[-1])\n",
    "        \n",
    "        dA.append(dZ[-1] @ self._W[1][1:,:].T)\n",
    "        dZ.append(dA[-1] * self._sigmoid_derivative(self.Z[0]))\n",
    "        dW.append(X.T @ dZ[-1])\n",
    "\n",
    "        dW.reverse()\n",
    "\n",
    "        return dW\n",
    "\n",
    "    def _weight_update(self, dW, curr_batch_size):\n",
    "        for i in range(self.size_of_hidden):\n",
    "            self._W[i] -= ( dW[i])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        _X = np.c_[np.ones((m, 1)), X]\n",
    "        y = y.to_numpy()\n",
    "        _y = y[:, np.newaxis]\n",
    "\n",
    "        self._W = []\n",
    "        self._W.append(self.rng.normal(scale=1, size=(n+1, self.hidden_layer_dimensions[0])))\n",
    "        for i in range(1, self.size_of_hidden):\n",
    "            self._W.append(self.rng.normal(scale=1, size=(self.hidden_layer_dimensions[i-1]+1, self.hidden_layer_dimensions[i])))\n",
    "        self._W.append(self.rng.normal(scale=1, size=(self.hidden_layer_dimensions[-1]+1, 1)))\n",
    "\n",
    "        n_batches = (n + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for b in range(n_batches):\n",
    "                _X_batch = _X[b * self.batch_size : (b + 1) * self.batch_size]\n",
    "                _y_batch = _y[b * self.batch_size : (b + 1) * self.batch_size]\n",
    "                y_pred = self._forward_propagation(_X_batch)\n",
    "                dW = self._backward_propagation(_X_batch, _y_batch)\n",
    "                curr_batch_size = _y_batch.shape[0]\n",
    "                self._weight_update(dW, curr_batch_size)\n",
    "            y_pred = self._forward_propagation(_X)\n",
    "            self.training_losses[epoch] = self.cost_function(_y, y_pred)\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        X = check_array(X)\n",
    "        n, m = X.shape\n",
    "        _X = np.c_[np.ones((n, 1)), X]\n",
    "        \n",
    "        pred_1 = self._forward_propagation(_X)\n",
    "        pred_0 = 1 - pred_1\n",
    "        return np.c_[pred_0, pred_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83      1496\n",
      "         1.0       0.84      0.82      0.83      1504\n",
      "\n",
      "    accuracy                           0.83      3000\n",
      "   macro avg       0.83      0.83      0.83      3000\n",
      "weighted avg       0.83      0.83      0.83      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetClass(hidden_layer_dimensions=[4, 4, 4, 4], learning_rate=0.0001, batch_size=32, epochs=100)\n",
    "model.fit(X_train_2, y_train_2)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_2 = model.predict(X_test_2)\n",
    "print(classification_report(y_test_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      1.00      0.67      1768\n",
      "         1.0       0.00      0.00      0.00      1732\n",
      "\n",
      "    accuracy                           0.51      3500\n",
      "   macro avg       0.25      0.50      0.34      3500\n",
      "weighted avg       0.26      0.51      0.34      3500\n",
      "\n",
      "[0 0 0 0 0] 2858    0.0\n",
      "1559    0.0\n",
      "1441    1.0\n",
      "2179    0.0\n",
      "1390    0.0\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programas\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Programas\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Programas\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y_pred_2[:5], y_train_2[:5])\n",
    "# print(y_pred_2.max())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2b3411e9fd7ba1867b3fa0e3f5ed61c5273131a2f49fe4d5db3319f94defb8e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
